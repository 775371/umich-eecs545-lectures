{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".warn {    \n",
       "    background-color: #ffff00;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 5px solid #dfb5b4;\n",
       "    padding: 0.5em;\n",
       "    }\n",
       " </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"files/spam.gif\" style=\"position:absolute; z-index: -5; vertical-align='middle'; left:100;\n",
    "right:0; margin-left:auto; margin-right:auto;opacity:0.3; filter:alpha(opacity=30);\"/>\n",
    "\n",
    "# EECS 545:  Machine Learning\n",
    "## Lecture 07:  Naive Bayes\n",
    "* Instructor:  **Jacob Abernethy**\n",
    "* Date:  February 1, 2016\n",
    "\n",
    "\n",
    "*Lecture Exposition Credit: Valliappa Chockalingam, Benjamin Bray*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Assume we have training data $(x^{(1)}, y^{(1)}), ..., (x^{(n)}, y^{(n)})$.\n",
    "\n",
    "As with a general classification problem: \n",
    "\n",
    "> Let $[x_1, ..., x_d]^T \\in \\mathbb{R}^d$ denote a feature vector.\n",
    "\n",
    "> Let $y \\in \\{0, ..., K\\}$ be a \"class label.\"\n",
    "\n",
    "Problem: Given a feature vector $x^{(i)}$ how can we predict it's corresponding class label $y^{(i)}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Motivating Example (<a href=\"http://images.google.com/images?num=30&amp;q=larry+bird\"> Kaggle Twitter Sentiment Analysis Corpus Data by UMich SI650 </a>) \n",
    "\n",
    "Aim: To create a classifier that can perform Sentiment Analysis (specifically classify sentences based on whether the opinion presented in them is positive or negative) \n",
    "\n",
    "\n",
    "Dataset:\n",
    "* Training set consisting of 7086 sentences classified as \"positive\" (1) or \"negative\" (0) \n",
    "* Testing set consists of 33052 unlabeled sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 1: Load the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "labels = [] # y, labels\n",
    "sentences = [] # inputs which will later be converted to feature vectors (x's)\n",
    "\n",
    "with open(\"training.txt\") as training_set_file: \n",
    "    for line in training_set_file: \n",
    "        # Based on the training set file, the lines are in following format: <label>\\t<sentence>\\n\n",
    "        # We can then separate the labels from the sentences using this format in mind.\n",
    "        labels.append(line.split(\"\\t\")[0])\n",
    "        sentences.append(line.split(\"\\t\")[1].replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sample of the data we have extracted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Brokeback Mountain was an AWESOME movie.', '1')\n",
      "('Harry Potter dragged Draco Malfoy \\xe2\\x80\\x99 s trousers down past his hips and sucked him into his throat with vigor, making whimpering noises and panting and groaning around the blonds rock-hard, aching cock...', '0')\n",
      "('These Harry Potter movies really suck.', '0')\n",
      "('This quiz sucks and Harry Potter sucks ok bye..', '0')\n",
      "(\"So as felicia's mom is cleaning the table, felicia grabs my keys and we dash out like freakin mission impossible.\", '1')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample = random.sample(range(0, len(sentences)), 5)\n",
    "\n",
    "for index in sample:\n",
    "    print(sentences[index], labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem: Since the sentences consist of a different number of words and different words even, how do we construct feature vectors that can represent sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Common Technique: \"Bag-of-words\" representation with present / not present boolean as the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Explanation: \n",
    "\n",
    "1) Construct a dictionary consisting of the d (unique) words from the training set. \n",
    "> Note that we don't have labels for words exclusively in the testing set and hence including words from the testing set won't really help us.\n",
    "\n",
    "2) Sentence $i$ is described using a feature vector $x^{(i)} = [x^{(i)}_1, ..., x^{(i)}_d]^T$ where $x_j^{(i)} = 1$ if the $j^\\text{th}$ word in the dictionary appears in the sentence and $x_j^{(i)} = 0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 2: Construct a dictionary and hence the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dictionary = []\n",
    "feature_vecs = []\n",
    "\n",
    "# Create a set of acceptable characters so that words with different capitalization, \n",
    "# additional punctuation or unicode are ignored (Simple data preprocessing)\n",
    "import string\n",
    "include = set(string.printable).difference(set.union(set(string.punctuation), set(string.digits)))\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split(\" \"): \n",
    "        # Get a unique word representation using the \"include\" set defined before\n",
    "        unique_word_rep = ''.join(ch for ch in word if ch.lower() in include).lower()\n",
    "        if unique_word_rep not in dictionary: \n",
    "            dictionary.append(unique_word_rep) \n",
    "        sentence.replace(word, unique_word_rep)\n",
    "\n",
    "for sentence in sentences:\n",
    "    feature_vec = []\n",
    "    for word in dictionary:\n",
    "        if word in sentence:\n",
    "            feature_vec.append(1)\n",
    "        else:\n",
    "            feature_vec.append(0)\n",
    "    feature_vecs.append(feature_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, what have we done? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excersizing\n",
      "exciting\n",
      "check\n",
      "hand\n",
      "calls\n"
     ]
    }
   ],
   "source": [
    "# Sample of words in the dictionary\n",
    "import random\n",
    "sample = random.sample(range(0, len(dictionary)), 5)\n",
    "\n",
    "for index in sample:\n",
    "    print(dictionary[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('da vinci code was an awesome movie...', '1')\n"
     ]
    }
   ],
   "source": [
    "# Sample of a feature vector (next slide) with corresponding sentence and label\n",
    "import random\n",
    "sample = random.sample(range(0, len(dictionary)), 1)\n",
    "\n",
    "print(sentences[index], labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# The big feature vector\n",
    "print(feature_vecs[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 3: Separate the feature vectors by class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "positive_feature_vecs = []\n",
    "negative_feature_vecs = []\n",
    "\n",
    "for i in range(len(feature_vecs)):\n",
    "    if labels[i] == \"1\":\n",
    "        positive_feature_vecs.append(feature_vecs[i])\n",
    "    else: \n",
    "        negative_feature_vecs.append(feature_vecs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Step 4: Compute Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"files/bayes-rule.png\" style=\"position:absolute; z-index: -5; vertical-align='middle'; left:100;\n",
    "right:0; margin-left:auto; margin-right:auto;\"/>\n",
    "\n",
    "Recall Bayes' Rule:\n",
    "\n",
    "$\\Pr(A \\mid B) = \\frac{\\Pr(B \\mid A)\\Pr(A)}{\\Pr(B)}$\n",
    "\n",
    "How can we use this?\n",
    "\n",
    "$\\Pr(y = k \\mid x) = \\frac{\\Pr(x \\mid y = k) \\Pr(y = k)}{\\Pr(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Things we need to find:\n",
    "> Prior class distribution: $\\forall k, Pr(y = k) \\hspace{0.2em}$\n",
    "\n",
    "> Likelihood: $Pr(x \\mid y = k)$\n",
    "\n",
    "> Denominator: $Pr(x)$? Not really needed. This probability does not depend on the label (y) and we are given the values of the features ($x_i$'s). Hence, this probability can be viewed as a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A visual representation of the same idea (note that the notation used here for feature vectors is $\\theta$ rather than $x$): \n",
    "\n",
    "<img src=\"files/naive_bayes_background.png\" style=\"position:relative; vertical-align='middle'; left:0;\n",
    "right:0; margin-left:auto; margin-right:auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's proceed with calculating the Maximum Likelihood estimates. \n",
    "\n",
    "Likelihood of seeing the training data is given by $\\prod \\limits_{i = 1}^n \\Pr(x^{(i)}, y^{(i)}) = \\prod \\limits_{i = 1}^n \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)})$\n",
    "\n",
    "<div class=\"warn\">\n",
    "*** Naive Bayes Assumption: *** $\\Pr(x \\mid y = k) = \\prod \\limits_{j = 1}^d \\Pr(x_j \\mid y = k)$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More mathematical explanation: Feature $x_j$ is conditionally independent of every other feature $x_k$ for $j \\neq k$. \n",
    "\n",
    "Conditional Independence: Two events R and B are conditionally independent given a third event Y if the occurrence or non-occurrence of R and the occurrence or non-occurrence of B are independent events in their conditional probability distribution given Y. \n",
    "\n",
    "In other words, R and B are conditionally independent given Y if and only if $\\Pr(R \\cap B \\mid F) = \\Pr(R \\mid F) \\Pr(B \\mid F)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Visual Explanation:\n",
    "\n",
    "<img src=\"files/cond_ind.png\" style=\"position:relative; vertical-align='middle'; left:0;\n",
    "right:0; margin-left:auto; margin-right:auto; height=50%\"/>\n",
    "            \n",
    "Each cell represents a possible outcome. The events R, B and Y are represented by the areas shaded red, blue and yellow respectively. The overlap between the events R and B is shaded purple. The probabilities of these events are shaded areas with respect to the total area. In both examples R and B are conditionally independent given Y because: $\\Pr(R \\cap B \\mid Y) =  \\Pr(R \\mid Y) \\Pr(B \\mid Y)$.\n",
    "\n",
    "More straightforward example based \"English\" explanation: A Naive Bayes classifier assumes that given a category (positive/negative tweet) whether it contains the word \"happy\" has no impact on whether it contains the word \"smile.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moving on...\n",
    "\n",
    "For the case where we have only two classes:\n",
    "\n",
    "$\\prod \\limits_{i = 1}^n \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) = $\n",
    "\n",
    "$\\left( \\prod \\limits_{i : y^{(i)} = 1} \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) \\right) \\cdot \\left( \\prod \\limits_{i : y^{(i)} = 0} \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With two classes, we further have that the prior probability follows a Bernoulli distribution, i.e., $\\Pr(y = k) \\sim Bernoulli(\\phi)$: \n",
    "\n",
    "1. Each observation represents only one of two outcomes: on observing a tweet it is classifies as a success (say positive sentiment tweet) or failure (say negative sentiment tweet)\n",
    "2. Mutual exclusion: each tweet belongs to one an only one class. \n",
    "3. Independence of trials: observing that one tweet is positive/negative doesn't have an effect over whether another tweet is positive/negative.\n",
    "4. Fixed probability of success/failure: On each trial, the probability of success is $\\phi$ and the probability of failure $1 − \\phi$ remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notation time!\n",
    "\n",
    "Let $N^+$ be the number of tweets classified as having a positive sentiment.\n",
    "\n",
    "Let $N^-$ be the number of tweets classified as having a negative sentiment.\n",
    "\n",
    "Let $N^+_j$ be the number of tweets classified as having a positive sentiment and containing word $j$.\n",
    "\n",
    "Let $N^-_j$ be the number of tweets classified as having a negative sentiment and containing word $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Multinomial Distribution\n",
    "\n",
    "- Generalization of the binomial distribution for n independent trials each of which leads to a success for exactly one of k categories with each category having a given fixed success probability.\n",
    "- The multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n",
    "\n",
    "Now, given a class label, each word in a document can be expressed as Multinomial variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think of it this way: Given that one has a positive tweet with $w$ words, there is going to be some distribution of the words (some words like \"awesome\" are more likely to appear than others like \"sad.\") \n",
    "\n",
    "Moreover, we have the Naive Bayes assumption of conditional independence of features. \n",
    "\n",
    "So, the positive tweet can be thought of as independent samplings of words given the distribution of words for the positive sentiment class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $\\Pr(x_j = 1 \\mid y = 1)$ (probability that a positive tweet has a given word) be $\\mu_j^+$. \n",
    "\n",
    "Let $\\Pr(x_j = 1 \\mid y = 1)$ (probability that a positive tweet has a given word) be $\\mu_j^-$.\n",
    "\n",
    "Then, $\\Pr(\\text{word} \\mid \\text{pos}) \\sim multinomial(\\mu_1^+, ..., \\mu_d^+)$ and \n",
    "\n",
    "$\\Pr(\\text{word} \\mid \\text{neg}) \\sim multinomial(\\mu_1^-, ..., \\mu_d^-)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall that we had: \n",
    "$\\prod \\limits_{i = 1}^n \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) = $\n",
    "\n",
    "$\\left( \\prod \\limits_{i : y^{(i)} = 1} \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) \\right) \\cdot \\left( \\prod \\limits_{i : y^{(i)} = 0} \\Pr(x^{(i)} \\mid y^{(i)}) \\Pr(y^{(i)}) \\right)$\n",
    "\n",
    "Now, $\\prod \\limits_{i : y^{(i)} = 1} \\Pr(y^{(i)}) = \\prod \\limits_{i : y^{(i)} = 1} \\phi = \\phi^{N^+}$ and \n",
    "\n",
    "$\\prod \\limits_{i : y^{(i)} = 0} \\Pr(y^{(i)}) = \\prod \\limits_{i : y^{(i)} = 0} (1 - \\phi) = \\phi^{N^-}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, $\\prod \\limits_{i : y^{(i)} = 1} \\Pr(x^{(i)} \\mid y^{(i)}) =  \\prod \\limits_{i : y^{(i)} = 1} \\prod \\limits_{j = 1}^d \\Pr(x_j^{(i)} = 1 \\mid y^{(i)} = 1) = \\prod \\limits_{j = 1}^d \\left(\\mu_j^+\\right)^{N_j^+}$ and \n",
    "\n",
    "$\\prod \\limits_{i : y^{(i)} = 0} \\Pr(x^{(i)} \\mid y^{(i)}) =  \\prod \\limits_{i : y^{(i)} = 0} \\prod \\limits_{j = 1}^d \\Pr(x_j^{(i)} = 1 \\mid y^{(i)} = 0) = \\prod \\limits_{j = 1}^d \\left(\\mu_j^-\\right)^{N_j^-}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, we now have the likelihood as: \n",
    "\n",
    "$\\left(\\phi^{N^+} \\prod \\limits_{j = 1}^d \\left(\\mu_j^+\\right)^{N_j^+} \\right) \\left((1 - \\phi)^{N^-} \\prod \\limits_{j = 1}^d \\left(\\mu_j^-\\right)^{N_j^-} \\right)$\n",
    "\n",
    "Log-likelhood is then given by: \n",
    "\n",
    "$N^+ \\cdot \\log \\phi + \\sum \\limits_{j = 1}^d {N_j^+} \\cdot \\log{(\\mu_j^+)} + $\n",
    "\n",
    "${N^-} \\cdot \\log (1 - \\phi) + \\sum \\limits_{j = 1}^d {N_j^-} \\cdot \\log{(\\mu_j^-)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximizing the log-likelihood by taking the derivative w.r.t. the parameters and setting them to 0:\n",
    "\n",
    "From $\\frac{\\partial l}{\\partial \\phi} = \\frac{1}{\\phi} N^+ - \\frac{1}{1 - \\phi}N^- = 0$, we have\n",
    "\n",
    "$\\phi = \\frac{N^+}{N^+ + N^-}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From conditional independence, we have: \n",
    "    \n",
    "$\\sum \\limits_{j = 1}^d N_j^+ \\cdot \\log{\\mu_j^+} = \\sum \\limits_{j = 1}^{d - 1} N_j^+ \\cdot \\log{\\mu_j^+} + N_d^+ \\log{\\left(1 - \\sum \\limits_{j = 1}^{d - 1} \\mu_j^+\\right)}$\n",
    "\n",
    "So, $\\frac{\\partial}{\\partial_j^+} \\left( \\sum \\limits_{j = 1}^d N_j^+ \\cdot \\log \\mu_j^+ \\right) = \\frac{N_j^+}{\\mu_j^+} - \\frac{N_d^+}{1 - \\sum_{j = 1}^{d - 1} \\mu_j^{+}}$\n",
    "\n",
    "$\\frac{N_j^+}{\\mu_j^+}$ is a constant, $\\forall j$. \n",
    "\n",
    "Thus, $\\mu_j^+ = \\frac{N_j^+}{\\sum_{j = 1}^d N_j^+}$\n",
    "\n",
    "Lot's of math just shown, let's apply it back to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior probability of being a positive tweet: 0.563787750494\n",
      "Prior probability of being a negative tweet: 0.436212249506\n"
     ]
    }
   ],
   "source": [
    "# Calculating the prior probabilities\n",
    "\n",
    "prior_prob_positive_tweet = float(sum(label == \"1\" for label in labels)) / len(labels)\n",
    "prior_prob_negative_tweet = 1 - prior_prob_positive_tweet\n",
    "\n",
    "print \"Prior probability of being a positive tweet: \" + str(prior_prob_positive_tweet)\n",
    "print \"Prior probability of being a negative tweet: \" + str(prior_prob_negative_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the probabilites for word existence | sentiment\n",
    "\n",
    "# Calculate the word counts for the positive sentiment class\n",
    "word_counts_for_pos_sentiment = {}\n",
    "\n",
    "for word in dictionary:\n",
    "    word_counts_for_pos_sentiment[word] = 0\n",
    "\n",
    "for pos_feature_vec in positive_feature_vecs: \n",
    "    for i in range(len(pos_feature_vec)):\n",
    "        if pos_feature_vec[i] == 1:\n",
    "            word_counts_for_pos_sentiment[dictionary[i]] += 1\n",
    "\n",
    "# Calculate the total number of distinct words in the positive sentiment class\n",
    "total_num_distinct_words_in_tweets_with_pos_sentiment = 0\n",
    "for word, word_count in word_counts_for_pos_sentiment.iteritems(): \n",
    "    total_num_distinct_words_in_tweets_with_pos_sentiment += word_count \n",
    "\n",
    "# Calculate the probability of the word for the positive setiment class\n",
    "word_probs_for_pos_sentiment = {}\n",
    "for word, word_count in word_counts_for_pos_sentiment.iteritems(): \n",
    "    word_probs_for_pos_sentiment[word] = float(word_count) / float(total_num_distinct_words_in_tweets_with_pos_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the word counts for the negative sentiment class\n",
    "word_counts_for_neg_sentiment = {}\n",
    "\n",
    "for word in dictionary:\n",
    "    word_counts_for_neg_sentiment[word] = 0\n",
    "\n",
    "for neg_feature_vec in negative_feature_vecs: \n",
    "    for i in range(len(neg_feature_vec)):\n",
    "        if neg_feature_vec[i] == 1:\n",
    "            word_counts_for_neg_sentiment[dictionary[i]] += 1\n",
    "                   \n",
    "\n",
    "# Calculate the total number of distinct words in the negative setiment class\n",
    "total_num_distinct_words_in_tweets_with_neg_sentiment = 0\n",
    "for word, word_count in word_counts_for_neg_sentiment.iteritems(): \n",
    "    total_num_distinct_words_in_tweets_with_neg_sentiment += word_count \n",
    "\n",
    "    \n",
    "# Calculate the probability of the word for the negative setiment class\n",
    "word_probs_for_neg_sentiment = {}\n",
    "for word, word_count in word_counts_for_neg_sentiment.iteritems(): \n",
    "    word_probs_for_neg_sentiment[word] = float(word_count) / float(total_num_distinct_words_in_tweets_with_neg_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the testing set \n",
    "testing_sentences = []\n",
    "\n",
    "with open(\"testdata.txt\") as testing_set_file: \n",
    "    for line in testing_set_file: \n",
    "        # Based on the training set file, the lines are in following format: <label>\\t<sentence>\\n\n",
    "        # We can then separate the labels from the sentences using this format in mind.\n",
    "        sentence = []\n",
    "        for word in line.replace(\"\\n\", \"\").split(\" \"):\n",
    "            sentence.append(''.join(ch for ch in word if ch.lower() in include).lower())\n",
    "        testing_sentences.append(sentence)\n",
    "\n",
    "# Just observing performance on a very small sample\n",
    "# Can test on the entire testing set to see performance (Optimizing this is the goal of the Kaggle Challenge!)\n",
    "import random\n",
    "sample = random.sample(range(0, len(testing_sentences)), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'im', 'gonna', 'go', 'enjoy', 'being', 'in', 'seattle'] 1\n",
      "['however', 'you', 'look', 'at', 'it', 'ive', 'been', 'here', 'almost', 'two', 'weeks', 'now', 'and', 'so', 'far', 'i', 'absolutely', 'love', 'ucla'] 1\n",
      "['why', 'i', 'love', 'volkswagen'] 1\n",
      "['i', 'should', 'remind', 'that', 'self', 'that', 'my', 'boring', 'green', 'toyota', 'camry', 'has', 'k', 'on', 'the', 'odometer', 'hasnt', 'been', 'to', 'get', 'a', 'tuneup', 'in', 'over', 'a', 'year', '', 'poor', 'thing'] 0\n",
      "['i', 'hate', 'george', 'w', 'bush', 'and', 'ignorance'] 0\n"
     ]
    }
   ],
   "source": [
    "prob_pos = {}\n",
    "for index in sample:\n",
    "    prob_pos[index] = prior_prob_positive_tweet\n",
    "    for word in dictionary:\n",
    "        if word in testing_sentences[index]:\n",
    "            prob_pos[index] *= word_probs_for_pos_sentiment[word]\n",
    "\n",
    "prob_neg = {}\n",
    "for index in sample:\n",
    "    prob_neg[index] = prior_prob_negative_tweet\n",
    "    for word in dictionary:\n",
    "        if word in testing_sentences[index]:\n",
    "            prob_neg[index] *= word_probs_for_neg_sentiment[word]\n",
    "\n",
    "# Show performance on the sample\n",
    "for index in sample: \n",
    "    if prob_pos[index] >= prob_neg[index]: \n",
    "        print testing_sentences[index], \"1\"\n",
    "    else: \n",
    "        print testing_sentences[index], \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's enter out own sentences now!\n",
    "\n",
    "ourOwnTests = []\n",
    "\n",
    "# Some sanity checks to see if the classifier has really learned something\n",
    "ourOwnTests.append(\"Happy Happy Happy.\")\n",
    "ourOwnTests.append(\"Love Love Love.\")\n",
    "\n",
    "ourOwnTests.append(\"Unhappy Unhappy Unhappy.\") \n",
    "ourOwnTests.append(\"Hate Hate Hate.\")\n",
    "\n",
    "# Testing what the classifier thinks of some of the movies and other pop culture references in the corpus\n",
    "ourOwnTests.append(\"Harry Potter.\") \n",
    "ourOwnTests.append(\"Missions Impossible.\")\n",
    "\n",
    "# Demonstrating the problems of NB classifiers\n",
    "ourOwnTests.append(\"Love Hate Love Hate.\")\n",
    "ourOwnTests.append(\"Happy Hate Happy Hate.\")\n",
    "\n",
    "# Some other random sentences to see how well the classifier performs\n",
    "ourOwnTests.append(\"Professor Abernethy juggles well.\")\n",
    "ourOwnTests.append(\"This sentence is false.\")\n",
    "ourOwnTests.append(\"Kernel Methods, Support Vector Machines, Neural Networks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy Happy Happy.\n",
      "Prob. pos: 8.20317632708e-09 , Prob. neg: 7.08294808462e-09 , Classification: +ive\n",
      "\n",
      "Love Love Love.\n",
      "Prob. pos: 2.04529662159e-07 , Prob. neg: 1.24919620695e-08 , Classification: +ive\n",
      "\n",
      "Unhappy Unhappy Unhappy.\n",
      "Prob. pos: 2.32159355182e-15 , Prob. neg: 2.29351618639e-15 , Classification: +ive\n",
      "\n",
      "Hate Hate Hate.\n",
      "Prob. pos: 2.94173606955e-10 , Prob. neg: 1.17863439973e-09 , Classification: -ive\n",
      "\n",
      "Harry Potter.\n",
      "Prob. pos: 6.95698276315e-07 , Prob. neg: 4.68644184565e-07 , Classification: +ive\n",
      "\n",
      "Missions Impossible.\n",
      "Prob. pos: 2.42335796426e-19 , Prob. neg: 3.80713174412e-21 , Classification: +ive\n",
      "\n",
      "Love Hate Love Hate.\n",
      "Prob. pos: 2.93870372946e-15 , Prob. neg: 1.00613533507e-15 , Classification: +ive\n",
      "\n",
      "Happy Hate Happy Hate.\n",
      "Prob. pos: 3.58278860038e-15 , Prob. neg: 1.72621943041e-14 , Classification: -ive\n",
      "\n",
      "Professor Abernethy juggles well.\n",
      "Prob. pos: 3.12714858799e-42 , Prob. neg: 7.80603124107e-43 , Classification: +ive\n",
      "\n",
      "This sentence is false.\n",
      "Prob. pos: 7.80028418633e-29 , Prob. neg: 4.32029376438e-31 , Classification: +ive\n",
      "\n",
      "Kernel Methods, Support Vector Machines, Neural Networks.\n",
      "Prob. pos: 1.02357053348e-53 , Prob. neg: 7.04102090726e-52 , Classification: -ive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prob_pos = {}\n",
    "for index in range(len(ourOwnTests)):\n",
    "    prob_pos[index] = prior_prob_positive_tweet\n",
    "    for word in dictionary:\n",
    "        if word in ourOwnTests[index]:\n",
    "            prob_pos[index] *= word_probs_for_pos_sentiment[word]\n",
    "\n",
    "prob_neg = {}\n",
    "for index in range(len(ourOwnTests)):\n",
    "    prob_neg[index] = prior_prob_negative_tweet\n",
    "    for word in dictionary:\n",
    "        if word in ourOwnTests[index]:\n",
    "            prob_neg[index] *= word_probs_for_neg_sentiment[word]\n",
    "\n",
    "for index in range(len(ourOwnTests)): \n",
    "    if prob_pos[index] >= prob_neg[index]: \n",
    "        print ourOwnTests[index]\n",
    "        print \"Prob. pos: \" + str(prob_pos[index]), \", Prob. neg: \" + str(prob_neg[index]), \", Classification: +ive\\n\"\n",
    "    else: \n",
    "        print ourOwnTests[index]\n",
    "        print \"Prob. pos: \" + str(prob_pos[index]), \", Prob. neg: \" + str(prob_neg[index]), \", Classification: -ive\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Happy Happy Happy.\n",
    "Prob. pos: 8.20317632708e-09 , Prob. neg: 7.08294808462e-09 , Classification: +ive\n",
    "\n",
    "Love Love Love.\n",
    "Prob. pos: 2.04529662159e-07 , Prob. neg: 1.24919620695e-08 , Classification: +ive\n",
    "\n",
    "Unhappy Unhappy Unhappy.\n",
    "Prob. pos: 2.32159355182e-15 , Prob. neg: 2.29351618639e-15 , Classification: +ive\n",
    "\n",
    "Hate Hate Hate. Classification: -ive\n",
    "Prob. pos: 2.94173606955e-10 , Prob. neg: 1.17863439973e-09 , Classification: -ive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mostly correct classification. One misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Professor Abernethy juggles well.\n",
    "Prob. pos: 3.12714858799e-42 , Prob. neg: 7.80603124107e-43 , Classification: +ive\n",
    "\n",
    "This sentence is false.\n",
    "Prob. pos: 7.80028418633e-29 , Prob. neg: 4.32029376438e-31 , Classification: +ive\n",
    "\n",
    "Kernel Methods, Support Vector Machines, Neural Networks. Classification: -ive\n",
    "Prob. pos: 1.02357053348e-53 , Prob. neg: 7.04102090726e-52 , Classification: -ive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: The probabilities are tiny! One would expect them to be 0 (see next slide), but we have float error to thank for the \"non-zeroness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice the problem? Particularly note the \"random\" sentences. \n",
    "\n",
    "What if a word is never present or is always present given a sentiment class? \n",
    "\n",
    "In the former case, suppose we get a sentence that has positive sentiment but a word in it has never appeared in any of the training set positive sentiment sentences, the classifier will assign it a probability of 0 for being in the positive sentiment class! This is because we are multiplying the probabilities of the words in the sentence.\n",
    "\n",
    "In the latter case, suppose we get a sentence that has negative sentiment but a word that has appeared in all of the training set negative sentiment sentences is not present, the classifier will assign it a probability of 0 for being in the negative sentiment class! Again, this is because we are multiplying the probabilities of the words in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One Solution: \"A Bayesian estimate of multinomial parameter with a Dirichlet prior.\" \n",
    "\n",
    "Mathematically, this says we do the following intuitive change:\n",
    "\n",
    "Instead of using $\\mu_j^+ = \\frac{N_j^+}{\\sum_{j = 1}^d N_j^+}$, use $\\mu_j^+ = \\frac{N_j^+ + 1}{\\sum_{j = 1}^d N_j^+ + K}$ where $K$ as you may recall is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analysis of NB classifiers\n",
    "\n",
    "Advantages:\n",
    "* Easy to implement (at least the Bernoulli NB type classifiers discussed above)\n",
    "* Can be easily extended:\n",
    "> * More classes: Multinomial NB Classifiers\n",
    "> * Continuous domains: Gaussian NB Classifier, assume $\\Pr(x_j \\mid y = k) \\sim Gaussian(...)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Disadvantages\n",
    "* Conditional Independence assumption can be false often times.\n",
    "\n",
    "A Note: While the classifier implemented above does not take into account frequencies of the word, one could use the frequencies of the words instead of a binary 0/1. This is the classic \"bag-of-words\" representation. Another representation to look at is <a href=https://code.google.com/p/word2vec/>word2vec</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminant Functions\n",
    "\n",
    "On the one hand, we have NB which assumes conditional indpendence of features. On the other we have discriminant functions that aim to find a function that takes as input the feature vectors and divide the inputs into classes (accurately)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way to classify inputs into classes: Linear Discriminant Functions\n",
    "\n",
    "Let $y(\\mathbf{x}) = \\mathbf{w^T x} + w_0$, $\\mathbf{w}$ is a \"weight\" vector and $w_0$ is a bias. \n",
    "\n",
    "* In 2-dimensions, this is a line.\n",
    "* In, 3 a plane. \n",
    "* In general, a \"hyperplane.\" More specifically, a separating hyperplane.\n",
    "\n",
    "In a 2 class scenario, classification can be done by assigning $\\mathbf{x}$ to class 1 is $y(\\mathbf{x}) \\geq 0$ and to class 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More than 2 classes (K > 2) scenario:\n",
    "\n",
    "Each class get's it's own function: $y_k(\\mathbf{x}) = \\mathbf{w}_k^T \\mathbf{x} + w_{k0}$.\n",
    "\n",
    "Assign $\\mathbf{x}$ to class k if $y_k(\\mathbf{x}) > y_j(\\mathbf{x}), \\forall j \\neq k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Decision Regions: convex polyhedra with piecewise linear boundaries.\n",
    "\n",
    "<img src=\"files/decision_regions.png\" style=\"position:relative; z-index: -5; vertical-align='middle'; left:0;\n",
    "right:0; margin-left:auto; margin-right:auto;opacity:1; filter:alpha(opacity=100);\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How to choose the weights vector?\n",
    "\n",
    "$\\mathbf{w}$ that minimizes squared error?\n",
    "\n",
    "Highly sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-bd62fbd75b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "\n",
    "fit = np.polyfit(x, y, 1)\n",
    "\n",
    "fit_fn = np.poly1d(fit) \n",
    "# fit_fn is now a function which takes in x and returns an estimate for y\n",
    "\n",
    "ax1.scatter(x, y, c=color, cmap=plt.cm.coolwarm)\n",
    "ax1.hold('on')\n",
    "ax1.plot(x, fit_fn(x), 'k')\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "\n",
    "x.append(10)\n",
    "y.append(-10)\n",
    "color.append(1)\n",
    "\n",
    "fit = np.polyfit(x, y, 1)\n",
    "\n",
    "fit_fn = np.poly1d(fit) \n",
    "# fit_fn is now a function which takes in x and returns an estimate for y\n",
    "\n",
    "ax2.scatter(x, y, c=color, cmap=plt.cm.coolwarm)\n",
    "ax2.hold('on')\n",
    "ax2.plot(x, fit_fn(x), 'k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fisher's Linear Discriminant\n",
    "\n",
    "Use $\\mathbf{w}$ to project $x$ onto one dimension\n",
    "\n",
    "If $\\mathbf{w}^T \\geq -w_0$ then assign $\\mathbf{x}$ to class 1 else to class 0.\n",
    "\n",
    "Select a $w$ that best \"separates\" the classes, i.e., \n",
    "\n",
    "* Maximizes class separation\n",
    "* Minimizes class variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Illustration when maximizing separation alone\n",
    "\n",
    "<img src=\"files/separation_alone.png\" style=\"position:relative; z-index: -5; vertical-align='middle'; left:0;\n",
    "right:0; margin-left:auto; margin-right:auto;opacity:1; filter:alpha(opacity=100);\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Illustration when maximizing separation and minimizing variance\n",
    "\n",
    "<img src=\"files/separation_and_variance.png\" style=\"position:relative; z-index: -5; vertical-align='middle'; left:0;\n",
    "right:0; margin-left:auto; margin-right:auto;opacity:1; filter:alpha(opacity=100);\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Objective Function \n",
    "\n",
    "Goal 1: Maximize the \"distance between classes\" (in particular, the distance between the projected means)\n",
    "\n",
    "$m_1 - m_0 \\equiv \\mathbf{w}^T (m_1 - m_0)$ where $\\mathbf{m}_k = \\frac{1}{N_k}\\sum_{i : y^{(i)} = k} \\mathbf{x}^(i)$ and $N_k$ is the number of inputs with class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Goal 2: Minimize the \"distance within each class\"\n",
    "\n",
    "$s_1^2 + s_0^2 \\equiv \\sum_{i : y^{(i)} = 0} (\\mathbf{w}^T\\mathbf{x}^{(i)} - m_1)^2 + \\sum_{i : y^{(i)} = 1} (\\mathbf{w}^T\\mathbf{x}^{(i)} - m_0)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Objective Function: $J(\\mathbf{w}) = \\frac{(m_1 - m_0)^2}{s_1^2 + s_0^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now make the dependence on $\\mathbf{w}$ the objective explicit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $S_\\mathbf{B}$ be the between-class covariance matrix given by $S_\\mathbf{B} = (\\mathbf{m}_1 - \\mathbf{m}_0)(\\mathbf{m}_1 - \\mathbf{m}_0)^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This matrix tells us how much the means of the different independent variables (features) covary (or how they are correlated without regard for scaling). In the 2-class case, $S_\\mathbf{B}$ reduces to $(\\mathbf{m}_1 - \\mathbf{m}_0)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let $S_\\mathbf{W}$ be the within-class covariance matrix given by $S_\\mathbf{W} = \\sum_{i : y^{(i)} = 0} (\\mathbf{x}^{(i)} - m_1)(\\mathbf{x}^{(i)} - m_1)^T + \\sum_{i : y^{(i)} = 1} (\\mathbf{w}^T\\mathbf{x}^{(i)} - m_0)(\\mathbf{w}^T\\mathbf{x}^{(i)} - m_0)^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This matrix tells us how much the means of the different independent variables (features) covary (or how they are correlated without regard for scaling). In the 2-class case, $S_\\mathbf{W}$ reduces to $\\sum_{i : y^{(i)} = 0} (\\mathbf{x}^{(i)} - m_1)^2 + \\sum_{i : y^{(i)} = 1} (\\mathbf{w}^T\\mathbf{x}^{(i)} - m_0)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The objective function can now be expressed as $J(\\mathbf{w}) = \\frac{\\mathbf{w}^T\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^T\\mathbf{S}_W\\mathbf{w}}$\n",
    "\n",
    "Differentaing w.r.t. $\\mathbf{w}$: $\\nabla_\\mathbf{w} J(\\mathbf{w}) = \\frac{\\nabla_\\mathbf{w} (\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w}) (\\mathbf{w}^T\\mathbf{S}_W\\mathbf{w}) - \\nabla_\\mathbf{w} (\\mathbf{w}^T\\mathbf{S}_W\\mathbf{w}) (\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w})}{(\\mathbf{w}^T\\mathbf{S}_W\\mathbf{w})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, $\\nabla_\\mathbf{w} (\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w}) = \\mathbf{S}_B \\mathbf{w}$ and $\\nabla_\\mathbf{w} (\\mathbf{w}^T \\mathbf{S}_W \\mathbf{w}) = \\mathbf{S}_W \\mathbf{w}$\n",
    "\n",
    "Additionally, the quadratic forms $\\mathbf{w}^T \\mathbf{S}_B \\mathbf{w}$ and $\\mathbf{w}^T \\mathbf{S}_W \\mathbf{w}$ are scalars. Furthermore, since $\\mathbf{S}_B = (\\mathbf{m}_1 - \\mathbf{m}_0) (\\mathbf{m}_1 - \\mathbf{m}_0)^T$, we have that $\\mathbf{S}_B\\mathbf{w}$ is in the direction of $(\\mathbf{m}_1 - \\mathbf{m}_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, setting the gradient to 0, we get that $\\mathbf{S}_B \\mathbf{w} (\\mathbf{w}^T\\mathbf{S}_W \\mathbf{w}) - \\mathbf{S}_W \\mathbf{w} (\\mathbf{w}^T\\mathbf{S}_W \\mathbf{w}) = 0$. Since we care only about the direction of $\\mathbf{w}$, we can ignore the constants and hence we have $w = c \\cdot \\mathbf{S}_w^{-1}(\\mathbf{m}_1 - \\mathbf{0})$ where $c$ is some constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Perceptron\n",
    "\n",
    "A \"generalized linear function\"\n",
    "\n",
    "* $y(x) = f(\\mathbf{w}^T \\phi(\\mathbf{x}))$\n",
    "\n",
    "$f(a) = \\begin{cases}\n",
    "+1 & a \\geq 0 \\\\\n",
    "-1 & a < 0 \\end{cases}$\n",
    "\n",
    "Let $t_{i} = +1$ if $x^{(i)}$ belongs to class 1 and $t_{i} = -1$ if $x^{(i)}$ belongs to class 0.\n",
    "\n",
    "Then, we always want $\\mathbf{w}^T \\phi(\\mathbf{x}^{(i)}) t_i > 0$ (the classification and true class should match)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron Criterion\n",
    "\n",
    "* Only count error from misclassified point:\n",
    "\n",
    "$E_P(w) = - \\sum \\limits_{x^{(i)} \\in \\mathcal{M}} \\mathbf{w}^T \\phi(\\mathbf{x}_n) t_i$ where $\\mathcal{M}$ are the misclassified points.\n",
    "\n",
    "* Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Update the weight vector according to the misclassified point (and only for the misclassified examples)\n",
    "\n",
    "$\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^\\tau - \\eta \\nabla_\\mathbf{w} E_P(\\mathbf{w}) = \\mathbf{w}^{(\\tau)} + \\eta \\phi(\\mathbf{x}^{(i)})) t_i$.\n",
    "\n",
    "(If $\\mathbf{x^{(i)}}$ is misclassified, add $\\phi(\\mathbf{x^{(i)}})$ into $\\mathbf{w}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os, subprocess\n",
    "    \n",
    "plt.ion()\n",
    "    \n",
    "class Perceptron:\n",
    "    def __init__(self, N):\n",
    "        # Random linearly separated data\n",
    "        xA,yA,xB,yB = [random.uniform(-1, 1) for i in range(4)]\n",
    "        self.V = np.array([xB*yA-xA*yB, yB-yA, xA-xB])\n",
    "        self.X = self.generate_points(N)\n",
    " \n",
    "    def generate_points(self, N):\n",
    "        X = []\n",
    "        for i in range(N):\n",
    "            x1,x2 = [random.uniform(-1, 1) for i in range(2)]\n",
    "            x = np.array([1,x1,x2])\n",
    "            s = int(np.sign(self.V.T.dot(x)))\n",
    "            X.append((x, s))\n",
    "        return X\n",
    "    \n",
    "    def plot(self, mispts=None, vec=None, save=False):\n",
    "        fig = plt.figure(figsize=(5,5))\n",
    "        plt.xlim(-1,1)\n",
    "        plt.ylim(-1,1)\n",
    "        V = self.V\n",
    "        a, b = -V[1]/V[2], -V[0]/V[2]\n",
    "        l = np.linspace(-1,1)\n",
    "        plt.plot(l, a*l+b, 'k-')\n",
    "        cols = {1: 'r', -1: 'b'}\n",
    "        for x,s in self.X:\n",
    "            plt.plot(x[1], x[2], cols[s]+'o')\n",
    "        if mispts:\n",
    "            for x,s in mispts:\n",
    "                plt.plot(x[1], x[2], cols[s]+'.')\n",
    "        if vec != None:\n",
    "            aa, bb = -vec[1]/vec[2], -vec[0]/vec[2]\n",
    "            plt.plot(l, aa*l+bb, 'g-', lw=2)\n",
    "        if save:\n",
    "            if not mispts:\n",
    "                plt.title('N = %s' % (str(len(self.X))))\n",
    "            else:\n",
    "                plt.title('N = %s with %s test points' \\\n",
    "                          % (str(len(self.X)),str(len(mispts))))\n",
    "            plt.savefig('p_N%s' % (str(len(self.X))), \\\n",
    "                        dpi=200, bbox_inches='tight')\n",
    " \n",
    "    def classification_error(self, vec, pts=None):\n",
    "        # Error defined as fraction of misclassified points\n",
    "        if not pts:\n",
    "            pts = self.X\n",
    "        M = len(pts)\n",
    "        n_mispts = 0\n",
    "        for x,s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                n_mispts += 1\n",
    "        error = n_mispts / float(M)\n",
    "        return error\n",
    " \n",
    "    def choose_miscl_point(self, vec):\n",
    "        # Choose a random point among the misclassified\n",
    "        pts = self.X\n",
    "        mispts = []\n",
    "        for x,s in pts:\n",
    "            if int(np.sign(vec.T.dot(x))) != s:\n",
    "                mispts.append((x, s))\n",
    "        return mispts[random.randrange(0,len(mispts))]\n",
    " \n",
    "    def pla(self, save=False):\n",
    "        # Initialize the weigths to zeros\n",
    "        w = np.zeros(3)\n",
    "        X, N = self.X, len(self.X)\n",
    "        it = 0\n",
    "        # Iterate until all points are correctly classified\n",
    "        while self.classification_error(w) != 0:\n",
    "            it += 1\n",
    "            # Pick random misclassified point\n",
    "            x, s = self.choose_miscl_point(w)\n",
    "            # Update weights\n",
    "            w += s*x\n",
    "            if save:\n",
    "                self.plot(vec=w)\n",
    "                plt.title('N = %s, Iteration %s\\n' \\\n",
    "                          % (str(N),str(it)))\n",
    "                plt.savefig('p_N%s_it%s' % (str(N),str(it)), \\\n",
    "                            dpi=200, bbox_inches='tight')\n",
    "        self.w = w\n",
    " \n",
    "    def check_error(self, M, vec):\n",
    "        check_pts = self.generate_points(M)\n",
    "        return self.classification_error(vec, pts=check_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as IPdisplay\n",
    "\n",
    "IPdisplay.Image(url=\"files/perceptron.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perceptron Convergence Theorem \n",
    "\n",
    "* If there exists an exact solution(i.e., the training data is linearly separable) the learning algortihm will find it in a finite number of steps.\n",
    "\n",
    "Limitations of Perceptron Learning\n",
    "\n",
    "* While the number of steps of the algorithm may be finite, the rate of convergence can be very slow.\n",
    "* If the dataset is not linearly separable, the learning algorithm will run forever and not convergence.\n",
    "* No clear generalization to more than 2 classes."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
