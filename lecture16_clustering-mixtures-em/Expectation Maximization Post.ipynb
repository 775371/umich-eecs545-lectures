{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scientific\n",
    "import numpy as np;\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl;\n",
    "from matplotlib import pyplot as plt;\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Font Size\n",
    "mpl.rcParams.update({\"font.size\" : 14});\n",
    "\n",
    "# python\n",
    "import random;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\LaTeX \\text{ macros here }\n",
    "\\newcommand{\\X}{\\mathcal{X}}\n",
    "\\newcommand{\\D}{\\mathcal{D}}\n",
    "\\newcommand{\\Z}{\\mathcal{Z}}\n",
    "\\newcommand{\\L}{\\mathcal{L}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, **Expectation-Maximization (EM)** is an iterative method for computing maximum likelihood estimates of the parameters in a statistical model, for data in which some variables are unobserved.\n",
    "\n",
    "This is my attempt to write *the tutorial I wish I had read* when I first started learning about Expectation-Maximization.  The algorithm is essential to all areas of machine learning, and it is crucial not only to have an intuitive picture of what the algorithm does, but also to master the mathematical details in order to derive procedures for new models.\n",
    "\n",
    "Below, I have tried to derive Expectation-Maximization as clearly and cleanly as possible, neglecting neither intuition nor rigor along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivation of Expectation-Maximization rests on the notions of *convexity* and *information*.  Let us briefly review these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we observe data $\\X$ generated from a model $p$ with parameters $\\theta$ in the presence of hidden variables $Z$.  As usual, we wish to compute the maximum likelihood estimate\n",
    "\n",
    "$$\n",
    "\\hat\\theta_{ML}\n",
    "= \\arg\\max_\\theta \\ell(\\theta|\\X)\n",
    "= \\arg\\max_\\theta \\ln p(\\X|\\theta)\n",
    "$$\n",
    "    \n",
    "of the parameters given our observed data.  In some cases, we may also seek to *infer* the values $\\Z$ of the hidden variables $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evidence Lower Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The data log-likelihood $\\ell(\\theta|\\X) = \\ln p(\\X|\\theta)$ of the parameters given the observed data is useful for both inference and parameter estimation.  Working directly with this quantity is often difficult in latent variable models, and so we must resort to other methods.\n",
    "\n",
    "Our general approach will be to reason about the hidden variables through a proxy distribution $q(z)$, which we use to compute a lower-bound on the log-likelihood.  This section is devoted to deriving one such bound, called the **Evidence Lower Bound (ELBO)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the ELBO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand the data log-likelihood by marginalizing over the hidden variables:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta|\\X)\n",
    "= \\ln p(\\X|\\theta)\n",
    "= \\ln \\sum_z p(\\X,z|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Jensen's inequality, we obtain the following bound, for any distribution $q(z)$ over the hidden variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\ell(\\theta | \\X)\n",
    "&=   \\ln \\sum_z p(\\X,z | \\theta) \\\\\n",
    "&=   \\ln \\sum_z q(z) \\frac{p(\\X, z | \\theta)}{q(z)} \\\\\n",
    "&\\geq \\sum_z q(z) \\ln \\frac{p(\\X,z | \\theta)}{q(z)}\n",
    "\\equiv \\L(q,\\theta)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lower bound $\\L(q, \\theta)$ is called the **Evidence Lower Bound (ELBO)** and can be rewritten as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\ell(\\theta | \\X)\n",
    "\\geq \\L(q, \\theta)\n",
    "&= \\sum_z q(z) \\ln \\frac{p(\\X,z | \\theta)}{q(z)} \\\\\n",
    "&= \\sum_z q(z) \\ln p(\\X, z | \\theta) - \\sum_z q(z) \\ln q(z) \\\\\n",
    "&= E_q[ \\ln p(\\X, Z | \\theta) ] - E_q[ \\ln q(z) ] \\\\\n",
    "&= E_q[ \\ln p(\\X, Z | \\theta) ] + H(q)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship to Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term in the last line above closely resembles the cross entropy between $q(Z)$ and the joint distribution $p(X,Z|\\theta)$ of the observed and hidden variables.  However, the variables $X$ are fixed to our observations $X = \\X$ and so $p(\\X, Z| \\theta)$ is an *unnormalized* ((In this case, $\\int p(\\X, z)\\, dz \\neq 1$.)) distribution over $Z$.  It is easy to see that this does not set us back too far; in fact, the lower bound $\\mathcal{L}(q,\\theta)$ differs from a Kullback-Liebler divergence only by a constant with respect to $Z$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "D_{KL}(q || p(Z|\\X,\\theta)\n",
    "&= H(q, p(Z|\\X,\\theta)) - H(q) \\\\\n",
    "&= E_q[ -\\ln p(Z|\\X,\\theta) ] - H(q) \\\\\n",
    "&= E_q[ -\\ln p(Z,\\X,\\theta) ] - E_q[ -\\ln p(\\X|\\theta) ] - H(q) \\\\\n",
    "&= E_q[ -\\ln p(Z,\\X,\\theta) ] + \\ln p(\\X|\\theta) - H(q) \\\\\n",
    "&= -\\mathcal{L}(q,\\theta) + \\mathrm{const.}\n",
    "\\end{align*}$$\n",
    "\n",
    "This yields a second proof of the evidence lower bound, following from the nonnegativity of relative entropy.  In fact, this is the proof given in **[tzikas2008:variational]** and **[murphy:mlapp]**.\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\ln(\\X | \\theta)\n",
    "= D_{KL}(q || p(Z,\\X,\\theta)) + \\mathcal{L}(q,\\theta) \n",
    "\\geq \\mathcal{L}(q,\\theta)\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:  Coin Flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[Do & Batzglou 2008]** Do, Chuong B. and Serafim Batzglou. _What is the Expectation Maximization Algorithm?_ Nature Biotechnology 26.8, August 2008.\n",
    "- **[Ruzzo]** Ruzzo, Larry.  _The Expectation Maximization Algorithm_. Lecture slides."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
