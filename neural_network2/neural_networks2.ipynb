{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import imp\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "def trim(im, percent=36):\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        x = im.crop(bbox)\n",
    "        return x.resize(((x.size[0]*percent)/100, (x.size[1]*percent)/100), Image.ANTIALIAS)\n",
    "\n",
    "def resize(filename, percent=36):\n",
    "    trim(Image.open(filename + \".png\"), percent).save(filename + \"_r\" + str(percent) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 20:  Neural Networks: Part 2\n",
    "* Instructor:  **Junhyuk Oh**\n",
    "* Date:  April 11, 2016\n",
    "- Many Slides/Figures/Examples/Ideas from:\n",
    "  - Nando De Freitas (University of Oxford)\n",
    "  - Richard Socher (MetaMind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- Deep Neural Networks\n",
    "  - Convolutional Neural Networks\n",
    "  - **Recurrent Neural Networks** \n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: Recurrent Neural Networks\n",
    "- Introduction\n",
    "- Simple RNN\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks (RNN)\n",
    "- A special kind of neural network designed for modeling sequential data\n",
    "  - Can take arbitrary number of inputs through time\n",
    "  - Can produce arbitrary number of outputs through time\n",
    "- Examples of sequential problems\n",
    "  - Next word prediction\n",
    "  - Machine translation\n",
    "  - Speech recognition\n",
    "  - Image caption generaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: Recurrent Neural Networks\n",
    "- Introduction\n",
    "- **Simple RNN**\n",
    "  - **Forward Propagation**\n",
    "  - Backward Propagation\n",
    "- Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple RNN\n",
    "$$ \\textbf{h}_t = f(\\textbf{W}\\textbf{x}_t + \\textbf{U}\\textbf{h}_{t-1}+\\textbf{b}) $$\n",
    "$$ \\hat{\\textbf{y}}_t = \\textbf{V}\\textbf{h}_t $$\n",
    "- $\\textbf{W}$: input weight, $\\textbf{U}$: recurrent weight, $\\textbf{V}$: output weight, $\\textbf{b}$: bias\n",
    "- $f$ is a non-linear activation function (e.g., ReLU)\n",
    "- Weights are shared across time: the number of parameters does not depend on the length of input/output sequence\n",
    "![](images/simple_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "![](images/simple_rnn_fprop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: Recurrent Neural Networks\n",
    "- Introduction\n",
    "- Simple RNN\n",
    "  - Forward Propagation\n",
    "  - **Backward Propagation**\n",
    "- Long Short-Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "resize(\"images/simple_rnn\", 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "- Assume that a loss function defined as:\n",
    "$$ \\mathcal{L} = \\sum_{t=1}^{T} \\mathcal{L}_t \\left( \\textbf{y}_t, \\hat{\\textbf{y}}_t \\right) $$\n",
    "- Gradient w.r.t. hidden units (given $\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{h}_{t+1}}$)\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\textbf{h}_t} = \\sum_{\\tau=t}^{T}\\frac{\\partial\\mathcal{L}_{\\tau}}{\\partial \\textbf{h}_t} \\mbox { } (\\because \\frac{\\partial\\mathcal{L}_k}{\\partial \\textbf{h}_t}=0 \\mbox { if } k < t)$$ \n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\textbf{h}_t} = \\frac{\\partial \\mathcal{L}_t}{\\partial \\textbf{h}_t} + \\frac{\\partial \\textbf{h}_{t+1}}{\\partial \\textbf{h}_{t}} \\frac{\\partial \\sum_{\\tau=t+1}^{T}\\mathcal{L}_{\\tau}}{\\partial \\textbf{h}_{t+1}}  \\\\\n",
    "= \\underbrace{\\frac{\\partial \\mathcal{L}_t}{\\partial \\hat{\\textbf{y}}_t}}_{\\mbox{easy}}\\underbrace{\\frac{{\\partial \\hat{\\textbf{y}}_t}}{\\partial \\textbf{h}_t}}_{\\mbox{easy}} + \\underbrace{\\frac{\\partial \\textbf{h}_{t+1}}{\\partial \\textbf{h}_{t}}}_{\\mbox{easy}} \\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{h}_{t+1}}}_{\\mbox{given}} $$\n",
    "![](images/simple_rnn_r70.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "- Assume that a loss function defined as:\n",
    "$$ \\mathcal{L} = \\sum_{t=1}^{T} \\mathcal{L}_t \\left( \\textbf{y}_t, \\hat{\\textbf{y}}_t \\right) $$\n",
    "- Gradient w.r.t. input units (given $\\frac{\\partial \\mathcal{L}}{\\partial \\textbf{h}_{t}}$)\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\textbf{x}_t} = \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{h}_t}\\frac{\\partial \\textbf{h}_t}{\\partial \\textbf{x}_t} $$\n",
    "![](images/simple_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "![](images/simple_rnn_back7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "resize(\"images/simple_rnn_back_w\", 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "- Gradient w.r.t. weights\n",
    "  - Note: the weights are shared through time\n",
    "  - Recall: we should accumulate gradients through time\n",
    "<font color='red'>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{V}} = \\sum_{t=1}^{T}\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\textbf{y}}_t}\\frac{\\partial \\hat{\\textbf{y}}_t}{\\partial \\textbf{V}} $$ </font>\n",
    "<font color='blue'>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{W}} = \\sum_{t=1}^{T}\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\textbf{h}}_t}\\frac{\\partial \\hat{\\textbf{h}}_t}{\\partial \\textbf{W}} $$ </font>\n",
    "<font color='green'>$$ \\frac{\\partial \\mathcal{L}}{\\partial \\textbf{U}} = \\sum_{t=1}^{T}\\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\textbf{h}}_t}\\frac{\\partial \\hat{\\textbf{h}}_t}{\\partial \\textbf{U}} $$ </font>\n",
    "![](images/simple_rnn_back_w_r60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backpropagation Through Time (BPTT)\n",
    "- BPTT is actually not different from backpropagation.\n",
    "- RNN is actually not much different from a standard (feedforward) neural network except that:\n",
    "  - Input/output are given through time.\n",
    "  - Weights are extensively shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline: Recurrent Neural Networks\n",
    "- Introduction\n",
    "- Simple RNN\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- **Long Short-Term Memory (LSTM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vanshing Gradient Problem\n",
    "- RNN can model arbitrary sequences if properly trained.\n",
    "- In practice, it is difficult to train it for long-term dependencies because of vanishing gradient.\n",
    "- Intuitively, a hidden unit does not affect other units in the long-term future due to new inputs.\n",
    "  - Gradients are diffused through time\n",
    "![](images/vanish_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Shor-Term Memory (LSTM)\n",
    "- A special type of RNN for handling **vanishing gradient** problem.\n",
    "- $c_t$ is a **memory cell** preserving information about history of inputs.\n",
    "- $h_t$ is the hidden activation which is given to the output layer.\n",
    "- $i_t,o_t,f_t$ are **input gate**, **output gate**, and **forget gate** respectively.\n",
    "![](images/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Shor-Term Memory (LSTM)\n",
    "- Gating mechanism control the following:\n",
    "  - whether to ignore a new input or not\n",
    "  - whether to produce an output or not (while preserving the memory cell)\n",
    "  - whether to erase the memory cell or not\n",
    "- Gating is controlled by LSTM's weights that are also learned from data.\n",
    "![](images/vanish_lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Shor-Term Memory (LSTM)\n",
    "- LSTM has been successfully applied to many sequence modeling problems."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
