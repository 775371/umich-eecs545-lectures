{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".boxed {\n",
       "    margin: 10px 30px;\n",
       "    padding: 10px;\n",
       "    border: 1px solid black;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<style>\n",
    ".boxed {\n",
    "    margin: 10px 30px;\n",
    "    padding: 10px;\n",
    "    border: 1px solid black;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like mass and energy, *information* is a fundamental quantity of the universe.  It may seem difficult at first to quantify information; we know that textbooks are dense with it and political speeches nearly devoid of it, but how can we formalize our intuition into something more useful?\n",
    "\n",
    "Claude Shannon (a Michigan undergrad!) set out to tackle this challenge, laying out the foundations of what we know today as **Information Theory** in his 1948 master's thesis, \"*A Mathematical Theory of Communication*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signals and Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we will mainly use information theory as a tool to manipulate probability distributions.  However, much of the intuition behind the theory comes from signal processing--indeed, this was Shannon's original purpose--so it is worthwhile to establish some basic terminology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"boxed\">\n",
    "*\"The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point.\"*  \n",
    "\n",
    "<div style=\"text-align:right\">&ndash; Claude E. Shannon</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Communication Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shannon outlines five key components of a communication system **[Shannon 1951]**:\n",
    "1. The **information source**, responsible for producing messages.  \n",
    "2. An **encoder**, which operates on the message in some way to produce a signal suitable for transmission.  \n",
    "3. A **channel**, along which messages are transmitted.  The signal may be partially or fully corrupted by *noise* at this stage.\n",
    "4. A **decoder**, which attempts to reconstruct the original message from the transmitted signal.\n",
    "5. The **destination** is the intended recipient of the original message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Noiseless Systems\n",
    "\n",
    "In these notes, we will focus on **discrete noisless systems**, in which both the message and signal are discrete, and where the channel introduces no noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information & Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the information we gain from receiving a message depends only on the probability $p$ of the message.  In other words, the *meaning* of the message does not matter to us, only the fact that the message was *selected from a set* of possible messages **[Shannon 1951]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axiomatic Derivation\n",
    "\n",
    "For information to be a useful quantity, it should satisfy the following axioms:\n",
    "\n",
    "1. Information is nonnegative, $I(p) \\geq 0$.  We can never *lose* information.\n",
    "2. A sure event provides no information, $I(1) = 0$.\n",
    "3. The information gained from observing two independent events is the sum of the information gained from observing each individually, $$I(p_1 * p_2) = I(p_1) + I(p_2)\n",
    "4. Information should be **continuous** and **monotonic**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Entropy\n",
    "\n",
    "- Distance between distributions\n",
    "- Signal interpretation\n",
    "- Example:  Morse code tailored to German text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- **[Shannon 1951]**\n",
    "- **[Pierce 1980]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
