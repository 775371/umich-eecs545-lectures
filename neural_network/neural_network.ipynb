{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import imp\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageChops\n",
    "\n",
    "def trim(im, percent=36):\n",
    "    bg = Image.new(im.mode, im.size, im.getpixel((0,0)))\n",
    "    diff = ImageChops.difference(im, bg)\n",
    "    diff = ImageChops.add(diff, diff, 2.0, -100)\n",
    "    bbox = diff.getbbox()\n",
    "    if bbox:\n",
    "        x = im.crop(bbox)\n",
    "        return x.resize(((x.size[0]*percent)/100, (x.size[1]*percent)/100), Image.ANTIALIAS)\n",
    "\n",
    "def resize(filename, percent=36):\n",
    "    trim(Image.open(filename + \".png\"), percent).save(filename + \"_r\" + str(percent) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# EECS 545:  Machine Learning\n",
    "## Lecture 22:  Neural Networks (Part 1)\n",
    "* Instructor:  **Junhyuk Oh**\n",
    "* Date:  April 6, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- Deep Neural Networks\n",
    "  - Convolutional Neural Networks\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Linear Classifiers\n",
    "\n",
    "- Linear classifiers (e.g., logistic regression) classify inputs based on linear combinations of input $x_i$\n",
    "- Many decisions involve non-linear functions of the input "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Limitations of Linear Classifiers\n",
    "- Canonical example (XOR function)\n",
    "  - The positive/negative examples are not *linearly separable*.\n",
    "  - Need to map the input ($x_1,x_2$) to a feature space where examples are linearly separable.\n",
    "<img src=images/xor.png width=400px />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Raquel Urtasun & Rich Zemel)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Representation\n",
    "<img src=images/motorbike_1.png width=1000px/>\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Slide Credit: Honglak Lee)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Representation\n",
    "<img src=images/motorbike_2.png width=1000px/>\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Slide Credit: Honglak Lee)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hand-crafted Feature Representation\n",
    "<img src=images/hand_crafted_pipeline.png width=1000px/>\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Slide Credit: Honglak Lee)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Drawbacks of Hand-crafted Feature Representation\n",
    "- Requires expert knowledge\n",
    "- Requires time-consuming hand-tuning <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Q) Can we learn useful features from raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\rightarrow$ Yes. That's what **deep learning** is trying to do. <br />\n",
    "$\\rightarrow$ ** Neural network** can implement the idea of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- **Basics of Neural Networks**\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- Deep Neural Networks\n",
    "  - Convolutional Neural Networks\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of Neural Networks\n",
    "- Input Layer: provides input\n",
    "- Hidden Layer: features extracted from input\n",
    "- Output Layer: output of the network\n",
    "- Parameters (or weights) for each layer\n",
    "<img src=\"images/nn.png\" width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of Neural Networks\n",
    "- A **loss function** is defined over the *output units* and *desired outputs* (i.e., labels)\n",
    "$$\\mathcal{L}\\left( \\textbf{y}, \\hat{\\textbf{y}} \\right) \\mbox{ where } \\hat{\\textbf{y}}=f(\\textbf{x};\\theta) \\mbox { } (\\textbf{h}^{(0)} \\equiv \\textbf{x})$$\n",
    "- The parameter of the network is trained to minimize the loss function based on gradient descent methods\n",
    "$$\\min_{\\theta} \\mathbb{E}_{(\\textbf{x},\\textbf{y}) \\sim \\mbox{data}} \\left[ \\mathcal{L}\\left( \\textbf{y}, \\hat{\\textbf{y}} \\right) \\right] \\mbox{ where } \\hat{\\textbf{y}}=f(\\textbf{x};\\theta) $$\n",
    "<img src=\"images/nn.png\" width=350px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of Neural Networks\n",
    "- Forward Propagation (inference): Compute $\\hat{\\textbf{y}}=f(\\textbf{x};\\theta)$ (output given input) \n",
    "- Backward Propagation (learning): Compute $\\nabla_{\\theta}\\mathcal{L}$ (gradient of loss w.r.t. parameters) \n",
    "    <img src=\"images/nn.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - **Forward Propagation**\n",
    "  - Backward Propagation\n",
    "- Deep Neural Networks\n",
    "  - Convolutional Neural Networks\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "<img src=\"images/nn_forward.png\" width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "<img src=\"images/nn_forward2.png\" width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "- The activation of each unit is computed based on **the previous layer** and **parameters (or weights)** associated with edges\n",
    "$$\\underbrace{\\textbf{h}^{(l)}}_{l\\mbox{-th layer}}=f^{(l)}(\\underbrace{\\textbf{h}^{(l-1)}}_{(l-1)\\mbox{-th layer}}; \\underbrace{\\theta^{(l)}}_{\\mbox{weights}})$$\n",
    "$$\\hat{\\textbf{y}}=f(\\textbf{x};\\theta)=f^{(L)} \\circ f^{(L-1)} \\cdots f^{(2)} \\circ f^{(1)}\\left(\\textbf{x} ; \\theta^{(1)} \\right) $$\n",
    "<img src=\"images/nn.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Layers: Linear\n",
    "$$ h_i=\\sum_{j}w_{ij}x_j + b_i$$\n",
    "$$ \\textbf{h} = \\textbf{W}\\textbf{x} + \\textbf{b} $$ \n",
    "- $\\textbf{x} \\in \\mathbb{R}^m $ : Input, $\\textbf{h} \\in \\mathbb{R}^n $ : Output \n",
    "- $\\textbf{W} \\in \\mathbb{R}^{n \\times m}$ : Weight, $\\textbf{b} \\in \\mathbb{R}^{n}$ : Bias $\\rightarrow$ parameter\n",
    "- Often called \"fully-connected layer\"\n",
    "<img src=\"images/linear.png\" width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Layers: Non-linear Activation Function\n",
    "- Applies a non-linear function to individual units.\n",
    "- There is no weight.\n",
    "- Allows neural networks to learn non-linear features.\n",
    "- ex) Sigmoid, Hyperbolic Tangent, Rectified Linear Function\n",
    "<img src=\"images/activation.png\" width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linear Activation: Sigmoid\n",
    "$$ h_i=\\sigma (x_i) = \\frac{1}{1+\\exp\\left(-x_i\\right)} $$\n",
    "$$ \\textbf{h} = \\sigma \\left(\\textbf{x} \\right) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25e0f35150>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndBGRiFBBFxGJiP8FBAgu5Y5jPHoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26003acf90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-5, 5, 100)\n",
    "plt.plot(xx, 1/(1 + np.exp(-1 * xx)), '-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linear Activation: Hyperbolic Tangent (Tanh)\n",
    "$$ h_i= \\mbox{tanh}(x_i)=\\frac{\\exp(x_i)-\\exp(-x_i)}{\\exp(x_i)+\\exp(-x_i)} $$\n",
    "$$ \\textbf{h} = \\mbox{tanh} \\left(\\textbf{x} \\right) $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25de437190>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkOUxD1LVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25e0f50a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-5, 5, 100)\n",
    "plt.plot(xx, (np.exp(xx) - np.exp(-1 * xx))/(np.exp(xx) + np.exp(-1 * xx)), '-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-linear Activation: Rectified Linear (ReLU)\n",
    "$$ h_i= \\mbox{ReLU}(x_i)=\\max\\left(x_i, 0 \\right) $$\n",
    "$$ \\textbf{h} = \\mbox{ReLU} \\left(\\textbf{x} \\right) $$ \n",
    "- Easier to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f25de355e50>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEACAYAAACatzzfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbmwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f25de46bd10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(-5, 5, 100)\n",
    "plt.plot(xx, xx * (xx > 0).astype(np.int), '-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Layers: Softmax\n",
    "$$ h_i = \\frac{\\exp(x_i)}{\\sum_{j}\\exp(x_j)} $$\n",
    "$$ \\textbf{h} = \\mbox{Softmax}(\\textbf{x}) $$\n",
    "- Note: $h_i \\geq 0$ and $\\sum_{i}h_i=1$\n",
    "- Useful for generating a multinomial distribution (classification)\n",
    "<img src=images/softmax.png width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layer Neural Network\n",
    "- Consists of multiple (linear + non-linear activation) layers.\n",
    "- Each layer learns non-linear features from its previous layer.\n",
    "- Often called Multi-Layer Perceptron (MLP).\n",
    "- 2-layer MLP with infinite number of hidden units can approximate any functions.\n",
    "<img src=\"images/mlp.png\" width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layer Neural Network\n",
    "- Simplified illustration that only shows edges with weights.\n",
    "- We assume that each layer is followed by a non-linear activation function (except for the output layer).\n",
    "<img src=\"images/mlp_standard.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layer Neural Network\n",
    "- More simplified illustration\n",
    "<img src=\"images/mlp_simple.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layer Neural Network\n",
    "- Even more simplified illustration\n",
    "<img src=\"images/mlp_simplest.png\" width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - Forward Propagation\n",
    "  - **Backward Propagation**\n",
    "- Deep Neural Networks\n",
    "  - Convolutional Neural Networks\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Neural Networks\n",
    "- Repeat until convergence\n",
    "  - $(\\textbf{x},\\textbf{y}) \\leftarrow$ Sample an example (or a mini-batch) from data\n",
    "  - $\\hat{\\textbf{y}} \\leftarrow f\\left( \\textbf{x} ; \\theta \\right)$ Forward propagation\n",
    "  - Compute $\\mathcal{L}\\left(\\textbf{y},\\hat{\\textbf{y}}\\right)$\n",
    "  - $\\nabla_{\\theta}\\mathcal{L} \\leftarrow$ Backward propagation\n",
    "  - Update weights using (stochastic) gradient descent\n",
    "      - $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta}\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Losses: Squared Loss\n",
    "$$ \\mathcal{L}\\left(\\textbf{y},\\hat{\\textbf{y}}\\right) = \\frac{1}{2}\\left\\Vert \\textbf{y}-\\hat{\\textbf{y}}\\right\\Vert^2_2 $$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_i}=\\hat{y}_i-y_i  \\iff \\nabla_{\\hat{\\textbf{y}}}\\mathcal{L}=\\hat{\\textbf{y}}-\\textbf{y} $$\n",
    "- Used for regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of Losses: Cross Entropy\n",
    "$$ \\mathcal{L}\\left(\\textbf{y},\\hat{\\textbf{y}}\\right) = -\\sum_{i}y_i\\log\\hat{y}_i $$\n",
    "$$ \\frac{\\partial\\mathcal{L}}{\\partial \\hat{y}_i}=-\\frac{y_i}{\\hat{y}_i} \\iff \\nabla_{\\hat{\\textbf{y}}}\\mathcal{L}=-\\frac{\\textbf{y}}{\\hat{\\textbf{y}}}$$\n",
    "\n",
    "- Measure a distance between two multinomial distributions\n",
    "- Used for classification problems \n",
    "  - ex) $\\textbf{y}=$[0 0 1], $\\hat{\\textbf{y}}=$[0.3 0.2 0.5], $\\mathcal{L}\\left(\\textbf{y},\\hat{\\textbf{y}}\\right)=-\\log 0.5$\n",
    "- When cross entropy is used with a softmax output, the last layer is equivalent to softmax regression (multi-class version of logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Neural Networks\n",
    "<img src=\"images/forward_backward.png\" width=700px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "- Denote $x,h,\\theta$ is the input, output, and parameter of a layer.\n",
    "- It is non-trivial to derive the gradient of loss w.r.t. parameters in intermediate layers\n",
    "<img src=images/idea_backprop.png width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "- Assuming that $\\frac{\\partial \\mathcal{L}}{\\partial h}$ is given, use the **chain rule** to compute the gradients\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta}=\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial h}}_{\\mbox{given}} \\underbrace{\\frac{\\partial h}{\\partial \\theta}}_{\\mbox{easy}} \\mbox{ , } \\frac{\\partial \\mathcal{L}}{\\partial x}=\\underbrace{\\frac{\\partial \\mathcal{L}}{\\partial h}}_{\\mbox{given}} \\underbrace{\\frac{\\partial h}{\\partial x}}_{\\mbox{easy}}$$\n",
    "- We need only $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ for gradient descent. Why compute $\\frac{\\partial \\mathcal{L}}{\\partial x}$? \n",
    "  - The previous layer needs it because $x$ is the output of the previous layer.\n",
    "<img src=images/idea_backprop2.png width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "<img src=\"images/nn_backward.png\" width=450px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "<img src=\"images/nn_backward_2.png\" width=450px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "<img src=\"images/nn_backward_3.png\" width=450px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea of Back-Propagation\n",
    "<img src=\"images/nn_backward_4.png\" width=450px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Back-Propagation Algorithm\n",
    "- Compute $\\nabla_{\\textbf{y}} \\mathcal{L}=\\left[\\frac{\\partial\\mathcal{L}}{\\partial y_1}, ..., \\frac{\\partial\\mathcal{L}}{\\partial y_n}\\right]$ directly from the loss function.\n",
    "- For each layer (from top to bottom) with output $\\textbf{h}$, input $\\textbf{x}$, and weights $\\textbf{W}$,\n",
    "  - Assuming that $\\nabla_{\\textbf{h}}\\mathcal{L}$ is given, compute gradients using the **chain rule** as follows: \n",
    "$$\\nabla_{\\textbf{W}}\\mathcal{L}=\\nabla_{\\textbf{h}}\\mathcal{L}\\nabla_{\\textbf{W}}\\textbf{h}$$\n",
    "$$\\nabla_{\\textbf{x}}\\mathcal{L}=\\nabla_{\\textbf{h}}\\mathcal{L}\\nabla_{\\textbf{x}}\\textbf{h}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice: Linear\n",
    "- Forward: $h_i=\\sum_{j}w_{ij}x_j + b_i \\iff  \\textbf{h} = \\textbf{W}\\textbf{x} + \\textbf{b}$ \n",
    "- Gradient w.r.t. parameters\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial h_{i}}\\frac{\\partial h_i}{\\partial w_{ij}}=\\frac{\\partial \\mathcal{L}}{\\partial h_{i}}x_j \\iff \\nabla_{\\textbf{W}} \\mathcal{L} = \\nabla_{\\textbf{h}}\\mathcal{L}\\textbf{x}^{\\top}$$\n",
    "$$ \\nabla_{\\textbf{b}} \\mathcal{L} = \\nabla_{\\textbf{h}}\\mathcal{L} $$\n",
    "- Gradient w.r.t. inputs\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial x_{j}} = \\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial h_{i}}\\frac{\\partial h_i}{\\partial x_{j}}=\\sum_{i} \\frac{\\partial \\mathcal{L}}{\\partial h_{i}}w_{ij} \\iff \\nabla_{\\textbf{x}} \\mathcal{L} = \\textbf{W}^{\\top}\\nabla_{\\textbf{h}}\\mathcal{L} $$\n",
    "<img src=\"images/linear_back.png\" width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practice: Sigmoid\n",
    "- Forward: $ h_i=\\sigma (x_i) = \\frac{1}{1+\\exp\\left(-x_i\\right)} \\iff \\textbf{h} = \\sigma \\left(\\textbf{x} \\right) $ \n",
    "- Gradient w.r.t. inputs\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial x_{i}} = \\frac{\\partial \\mathcal{L}}{\\partial h_{i}}\\frac{\\partial h_i}{\\partial x_{i}}=\\frac{\\partial \\mathcal{L}}{\\partial h_{i}}\\sigma(x_i)(1-\\sigma(x_i))=\\frac{\\partial \\mathcal{L}}{\\partial h_{i}}h_i(1-h_i)$$\n",
    "$$ \\nabla_{\\textbf{x}} \\mathcal{L} = \\nabla_{\\textbf{h}}\\mathcal{L} \\odot \\textbf{h} \\odot (\\textbf{1} - \\textbf{h}) $$\n",
    "<img src=\"images/activation_back.png\" width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Code (Torch/Lua)\n",
    "```lua\n",
    "require \"torch\"\n",
    "require \"nn\"\n",
    "\n",
    "dataX = torch.Tensor(1000, 3)\n",
    "dataY = torch.Tensor(1000):random(2)\n",
    "-- network construction\n",
    "-- 3 -> 4 -> 3 -> 2\n",
    "model = nn.Sequential()\n",
    "model:add(nn.Linear(3,4))\n",
    "model:add(nn.Sigmoid())\n",
    "model:add(nn.Linear(4,3))\n",
    "model:add(nn.Sigmoid())\n",
    "model:add(nn.Linear(3,2))\n",
    "model:add(nn.LogSoftMax())\n",
    "-- loss function (cross entropy)\n",
    "criterion = nn.ClassNLLCriterion() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```lua\n",
    "for iter=1,10000 do\n",
    "    -- sample an example from a dataset\n",
    "    idx = torch.random(1000)\n",
    "    x = dataX[idx]\n",
    "    y = dataY[idx]\n",
    "    -- forward propagation\n",
    "    y_pred = model:forward(x)\n",
    "    loss = criterion:forward(y_pred, y)\n",
    "    -- backward propagation\n",
    "    model:zeroGradParameters() -- dL/dW = 0\n",
    "    -- compute dL/dy\n",
    "    dL_dy = criterion:backward(y_pred, y) \n",
    "    -- compute gradient w.r.t. weights\n",
    "    model:backward(x, dL_dy) \n",
    "    -- SGD update with learning rate of 0.001\n",
    "    model:updateParameters(0.001) \n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FAQ: Does a neural network have to be always layer-structured?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No. It can be any directed acyclic graph (DAG).\n",
    "- Example of a complex neural network\n",
    "<img src=\"images/dag.png\" width=300px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FAQ: Can we define any arbitrary layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can define any layer as long as it is differentiable.\n",
    "- Example) Addition layer\n",
    "  - Forward: $\\textbf{h} = \\textbf{x}_1 + \\textbf{x}_2$\n",
    "  - Backward\n",
    "    - $\\nabla_{\\textbf{x}_1}\\mathcal{L} = \\nabla_{\\textbf{h}}\\mathcal{L}\\nabla_{\\textbf{x}_1}\\textbf{h}=\\nabla_{\\textbf{h}}\\mathcal{L}$\n",
    "    - $\\nabla_{\\textbf{x}_2}\\mathcal{L} = \\nabla_{\\textbf{h}}\\mathcal{L}\\nabla_{\\textbf{x}_2}\\textbf{h}=\\nabla_{\\textbf{h}}\\mathcal{L}$\n",
    "<img src=\"images/addition.png\" width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FAQ: How to handle shared weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To constrain $W_1=W_2=W$, we need $\\Delta W_1 = \\Delta W_2$.\n",
    "- Compute $\\nabla_{W_1}\\mathcal{L}$ and $\\nabla_{W_2}\\mathcal{L}$ separately.\n",
    "- Use $\\nabla_{W}\\mathcal{L}=\\nabla_{W_1}\\mathcal{L}+\\nabla_{W_2}\\mathcal{L}$ to update the shared weight.\n",
    "- In practice, we accumulate gradients to the shared memory space for $\\nabla_{W}\\mathcal{L}$ during back-propagation.\n",
    "- Weight sharing is used in *convolutional neural networks* and *recurrent neural networks*.\n",
    "<img src=\"images/weight_sharing.png\" width=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- **Deep Neural Networks**\n",
    "  - Convolutional Neural Networks\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"deep\" neural network?\n",
    "- A neural network is considered to be **deep** if it has more than two (non-linear) hidden layers..\n",
    "- Higher layers extract more abstract and hierarchical features.\n",
    "  - In a classification network, features become more *linearly separable* as layer goes up\n",
    "<img src=\"images/deep_nn.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is \"deep\" neural network?\n",
    "- Difficulties in training deep neural networks\n",
    "  - Easy to overfit (The number of parameters is large)\n",
    "  - Hard to optimize (highly non-convex optimization)\n",
    "  - Computationally expensive (many matrix multiplications)\n",
    "- Recent Advances\n",
    "  - Large-scale dataset (e.g., 1M images in ImageNet)\n",
    "  - Better regularization (e.g., Dropout)\n",
    "  - Better optimization (e.g., RMSProp, ReLU) \n",
    "  - Better hardware (GPU for matrix computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Popular Deep Architectures\n",
    "- Convolutional Neural Network (CNN)\n",
    "  - Widely used for image modeling\n",
    "  - ex) object recognition, segmentation, vision-based reinforcement learning problems\n",
    "- Recurrent Neural Network (RNN)\n",
    "  - Widely used for sequential data modeling\n",
    "  - ex) machine translation, image caption generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outline\n",
    "\n",
    "- Motivation\n",
    "- Basics of Neural Networks\n",
    "  - Forward Propagation\n",
    "  - Backward Propagation\n",
    "- Deep Neural Networks\n",
    "  - **Convolutional Neural Networks**\n",
    "  - Recurrent Neural Networks\n",
    "- Applications\n",
    "  - Computer Vision\n",
    "  - Natural Language Processing\n",
    "  - Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neural Network\n",
    "- A special kind of multi-layer neural network\n",
    "- Designed to recognize visual patterns directly from raw pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layer Perceptron (MLP)\n",
    "- Consider 100x100 input pixels\n",
    "- 1 hidden layer with 10000 hidden units\n",
    "- **100M parameters** $\\rightarrow$ infeasible!\n",
    "$\\rightarrow$ Pixels are locally correlated!\n",
    "<img src=\"images/cnn_ex1.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Locally-connected Neural Network\n",
    "- Consider 100x100 input pixels\n",
    "- Each unit is connected to 10x10 pixels.\n",
    "- Each unit extracts a local pattern from the image.\n",
    "- 10000 hidden units\n",
    "- **1M parameters** $\\rightarrow$ still too large\n",
    "<img src=\"images/cnn_ex2.png\" width=400px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neural Network (one filter)\n",
    "- Consider 100x100 input pixels\n",
    "- Apply the **same filter (weight)** over the entire image.\n",
    "- Hidden units form a 100x100 **feature map**.\n",
    "- 10x10 parameters. <br />\n",
    "$\\rightarrow$ **only captures a single local pattern.**\n",
    "<img src=\"images/cnn_ex3.png\" width=700px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Convolutional Neural Network (multiple filters)\n",
    "- 100x100 input pixels\n",
    "- Apply K number of 10x10 filters. \n",
    "- Hidden units form a Kx100x100 feature map.\n",
    "- Kx10x10 parameters. <br />\n",
    "- Num of filters and size of filters are hyperparameters.\n",
    "<img src=\"images/cnn_ex4.png\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Typical Deep CNN Architecture\n",
    "<img src=\"images/typical_cnn.png\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution\n",
    "<img src=\"images/convolution_step1.png\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution: Convolutional Filtering\n",
    "- A filter has $\\textbf{W} \\in \\mathbb{R}^{h \\times w}$ weights (bias is omitted for simplicity).\n",
    "- Compute inner products between $\\textbf{W}$ and $h \\times w$ input patches by sliding window.\n",
    "  - The same weight is shared across the entire image.\n",
    "- The following animation shows the simplest case: one-channel input, one filter\n",
    "<img src=images/convolution.gif width=350px />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Stanford UFLDL Tutorial)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution: Convolutional Filtering\n",
    "- In general, an input consists of multiple channels.\n",
    "  - Each filter has $\\textbf{W} \\in \\mathbb{R}^{c \\times h \\times w}$ weight ($c$: \\#channels of input).\n",
    "- Applying $K$ different filters $\\rightarrow$ produces a 3D feature map (stacked through channels).\n",
    "- Hyperparameters: num of filters, size of filters\n",
    "- The following figure shows the general case: multi-channel input, mutilple filters\n",
    "<img src=images/convolution_multi.png width=500px />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Yann LeCun)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution: Non-linearity\n",
    "- Method: Just apply non-linear function (e.g., Sigmoid, ReLU)\n",
    "- ReLU is preferred because it is easier to optimize.\n",
    "<img src=\"images/convolution_step2.png\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution: Pooling\n",
    "<img src=\"images/convolution_step3.png\" width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Details of Convolution: Pooling\n",
    "- Method: Take average or maximum over HxW region of input feature.\n",
    "- Outcome\n",
    "  - Shrink the number of hidden units.$\\rightarrow$ reduces the number of parameters at the end.\n",
    "  - Make features robust to small translations of image.\n",
    "- Hyperparameters: pooling method (avg or max), pooling size\n",
    "- Often called \"sub-sampling\"\n",
    "<img src=images/pooling.gif width=500px />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Stanford UFLDL Tutorial)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustration of Deep CNN\n",
    "- Feature maps become smaller in higher layers due to pooling.\n",
    "- hidden units in higher layers capture patterns from larger input patches.\n",
    "<img src=images/deep_cnn.png />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Yann LeCun)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Code (Torch/Lua)\n",
    "```lua\n",
    "require \"torch\"\n",
    "require \"nn\"\n",
    "model = nn.Sequential()\n",
    "-- 1st convolution: 3 -> 64 (5x5 filters)\n",
    "model:add(nn.SpatialConvolution(3, 64, 5, 5))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(2,2,2,2)) -- 2x2 pooling\n",
    "-- 2nd convolution: 64 -> 128 (5x5 filters)\n",
    "model:add(nn.SpatialConvolution(64, 128, 5, 5))\n",
    "model:add(nn.ReLU())\n",
    "model:add(nn.SpatialMaxPooling(2,2,2,2))\n",
    "-- Reshape 3D map (128*5*5) to a long vector\n",
    "model:add(nn.View(-1):setNumInputDims(3)) \n",
    "-- 3rd linear: 128*5*5 -> 128 hidden units\n",
    "model:add(nn.Linear(128*5*5, 128))\n",
    "model:add(nn.ReLU())\n",
    "-- 4th linear: 128 -> 10 classes\n",
    "model:add(nn.Linear(128, 10))\n",
    "model:add(nn.LogSoftMax()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is learned by CNN? Filter Visualization\n",
    "- Train a deep CNN on *ImageNet* (1.2M images, 1000 classes)\n",
    "- Perform forward propagations from many examples\n",
    "- Find image patches that strongly activate a specific feature map (filter)\n",
    "- Reconstruct the input patch from the feature map\n",
    "- Proposed by Zeiler and Fergus (ECCV 2014)\n",
    "<img src=images/deep_cnn.png />\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Yann LeCun)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Visualization: 1st and 2nd Layer\n",
    "<img src=images/cnn_visualization1.png width=700px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Visualization: 3rd Layer\n",
    "- Shows more complex patterns\n",
    "<img src=images/cnn_visualization2.png width=1000px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Visualization: 4th Layer\n",
    "- More class-specific\n",
    "<img src=images/cnn_visualization3.png width=1000px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter Visualization: 5th Layer\n",
    "- Shows entire objects with pose variations\n",
    "- Each filter can be viewed as a part detector. (e.g., dog face, text, animal leg)\n",
    "<img src=images/cnn_visualization4.png width=1000px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Comparison to Traditional Approach\n",
    "<img src=images/cnn_comparison.png width=1000px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ImageNet Challenge 2012\n",
    "- AlexNet (7 layers, Krizhevsky et al.) achieved 16.4% error.\n",
    "- The next best model (non-CNN) achieved 26.2% error.\n",
    "![](images/imagenet.jpg)\n",
    "![](images/alexnet.png)\n",
    "<span style=\"color:gray; font-size:10px; float:right\">(Figure from Alex Krizhevsky et al.)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ImageNet Challenge\n",
    "- ImageNet 2013: Clarifi (7 layers) $\\rightarrow$ 14.8% error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ImageNet 2014: GoogLeNet (22 layers) $\\rightarrow$ 4.9% error (Human: 5.1% error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ImageNet 2015: ResNet (152 layers!) $\\rightarrow$ 3.5% error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Feature Generalization: Dataset\n",
    "- Pre-train a CNN on a large-scale dataset (ImageNet) and **train only the final linear layer** on another dataset.\n",
    "- Achieves state-of-the-art results on small datasets (e.g., Caltech-101).\n",
    "- The learned features can generalize to any dataset.\n",
    "<img src=images/cnn_generalization1.png width=600px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Generalization: Task\n",
    "- There is a large-scale dataset (ImageNet) for classification, but there are only small-scale datasets for other tasks (e.g., detection, segmentation).\n",
    "- Pre-train a CNN on a large-scale classification dataset.\n",
    "- Use the pre-trained CNN as \"feature extraction\" method for other tasks.\n",
    "<img src=images/cnn_generalization2.png width=800px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Generalization: Task\n",
    "- Achieves state-of-the-art results on many other vision tasks.\n",
    "  -  ex) object detection, segmentation, depth map prediction, image caption generation\n",
    "- A CNN trained on a large-scale classification dataset learns a generic feature that can be used for many different vision tasks.\n",
    "<img src=images/cnn_generalization3.png width=900px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of Convolutional Neural Network\n",
    "- Convolutional Neural Network: a special kind of neural network with local connectivity and weight sharing.\n",
    "- Achieves state-of-the-art performances on many different computer visoin tasks\n",
    "- Higher layers extract high-level features (e.g., dog).\n",
    "- Learned features can be generally used for other vision tasks."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
